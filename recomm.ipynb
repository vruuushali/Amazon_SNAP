{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set environment variables for py4jjava error\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON']= sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']= sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession object\n",
    "spark= SparkSession.builder.appName(\"Amazon_get_recomm\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.timeout\", \"300\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from a .txt file and create a DataFrame\n",
    "metadf = spark.read.options().text(r\"C:\\Users\\vrushalideshmukh\\Documents\\BFL_Internship_Docs\\amazon-meta.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list called 'listing'\n",
    "listing = []\n",
    "#Open the file at the specified path and encode it using utf-8\n",
    "# fileopen = open(r\"C:\\Users\\vrushalideshmukh\\Documents\\BFL_Internship_Docs\\amazon-meta.txt\",encoding='utf-8')\n",
    "fileopen = open(r\"C:\\Users\\vrushalideshmukh\\Documents\\BFL_Internship_Docs\\amazon-meta.txt\",encoding='utf-8')\n",
    "#Start an infinite loop until there are no more lines to read in the file\n",
    "while True:\n",
    "    line = fileopen.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    else: listing.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# create an empty list\n",
    "dict=[]\n",
    "# initialize flag to 0\n",
    "flag = 0\n",
    "# initialize newstr variable to empty string\n",
    "newstr = \"\"\n",
    "\n",
    "# loop through each line in the listing\n",
    "for i in listing:\n",
    "    # check if the line contains \"Id\"\n",
    "    if \"Id\" in i:\n",
    "        # check if flag is 0\n",
    "        if flag == 0:\n",
    "            newstr= \"\" + i\n",
    "            flag = 1\n",
    "            continue\n",
    "        # check if flag is 1\n",
    "        elif flag == 1:\n",
    "            # remove trailing newline character\n",
    "            if newstr.endswith('\\n'):\n",
    "                newstr = newstr[:-1]\n",
    "            dict.append([newstr])\n",
    "            newstr=\"\"\n",
    "    newstr += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdict=[]\n",
    "newdict= dict\n",
    "# create a spark dataframe from first 50 tuples\n",
    "metadf = spark.createDataFrame(dict[:1000],['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions module and alias it as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define regular expressions as string literals\n",
    "product_id_regex = '^Id:\\s*(\\d+)'\n",
    "product_ASIN_regex = '.*(?:ASIN:\\s*)(\\d+|\\w+)'\n",
    "product_title_regex = 'title:\\s*(.*)'\n",
    "product_group_regex = '.*(?:group:\\s*)([\\w]*)'\n",
    "sales_rank_regex = '.*(?:salesrank:\\s*)(\\d*)'\n",
    "similar_products_regex = '.*(?:similar:\\s*[1-9]\\s*)([\\w ?]*)'\n",
    "categories_regex = 'categories:\\s*[1-9]\\s*([\\s\\S]*?)(?=reviews|\\Z)'\n",
    "# reviews_regex = '.*(?:reviews:\\\\s*)(\\\\d+),(\\\\d+),([\\\\d.]+),(.*)'\n",
    "total_reviews_regex = '.*(?:reviews:\\s*)total:\\s*(\\d+)'\n",
    "avg_rating_regex = '.*avg rating:\\s*([\\d.]+)'\n",
    "# custdata_regex = '\\n*\\r*\\s*(\\d*-\\d*-\\d*)\\s*cutomer:\\s*([A-Za-z0-9]*)\\s*rating:\\s*(\\d*)\\s*votes:\\s*(\\d*)\\s*helpful:\\s*(\\d*)\\r*\\n*'\n",
    "\n",
    "# list of metadata field names\n",
    "metadata_fields = ['id', 'asins', 'titles', 'groups', 'salesranks', 'categories', 'similars','total_reviews', 'avg_rating']\n",
    "\n",
    "# loop through each regular expression and field name\n",
    "for i, regex in enumerate([product_id_regex, product_ASIN_regex, product_title_regex, product_group_regex, sales_rank_regex, categories_regex, similar_products_regex, total_reviews_regex, avg_rating_regex]):\n",
    "    # get the corresponding field name from the metadata_fields list\n",
    "    field_name = metadata_fields[i]\n",
    "    # apply regular expression and create a new column with the field name\n",
    "    metadf = metadf.withColumn(field_name, F.regexp_extract('value', regex, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'similars' column by whitespace and convert to list\n",
    "split_col = F.split(metadf['similars'], '\\\\s+')\n",
    "\n",
    "# Define a user-defined function (UDF) to convert the split column to a list\n",
    "# This lambda function simply takes a single input argument x and returns it unchanged.\n",
    "# The purpose of the UDF is to apply a transformation to the input data, and the transformation is defined by the lambda function.\n",
    "# The UDF is being used to convert the split column (which is a column of strings) to a list. \n",
    "# The lambda function simply returns the input string unchanged, so the UDF applies no transformation to the data. \n",
    "# However, because we have specified that the return type of the UDF is ArrayType(StringType())\n",
    "# Hence, the resulting output column will be a column of lists of strings.\n",
    "to_list_udf = F.udf(lambda x: x, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the split column and create a new column with the resulting list\n",
    "metadf = metadf.withColumn('similars_list', to_list_udf(split_col))\n",
    "\n",
    "#The resulting 'similars_list' column will contain a list of strings that were previously separated by whitespace in the 'similars' column.\n",
    "# Drop the original 'similars' column since we no longer need it\n",
    "metadf=metadf.drop('similars')\n",
    "\n",
    "# Split categories by '\\n' and create new columns to separate categories\n",
    "metadf = metadf.withColumn('categories_list', F.split(metadf['categories'], '\\n'))\n",
    "metadf = metadf.withColumn('category_1', metadf['categories_list'][0])\n",
    "metadf = metadf.withColumn('category_2', metadf['categories_list'][1])\n",
    "metadf = metadf.withColumn('category_3', metadf['categories_list'][2])\n",
    "metadf = metadf.withColumn('category_4', metadf['categories_list'][3])\n",
    "metadf = metadf.withColumn('category_5', metadf['categories_list'][4])\n",
    "\n",
    "# Use a conditional statement to check if there is a 6th category, and create a new column for it if there is\n",
    "metadf = metadf.withColumn('category_6', F.when(F.size(metadf['categories_list']) > 1, metadf['categories_list'][5]).otherwise(None))\n",
    "\n",
    "# Drop the 'categories_list' column since we no longer need it\n",
    "# metadf = metadf.drop('categories_list')\n",
    "\n",
    "# Drop the original 'categories' column since we've extracted the relevant information into new columns\n",
    "metadf=metadf.drop('categories')\n",
    "\n",
    "# Drop the 'value' column since it was only used temporarily to extract the metadata fields\n",
    "\n",
    "metadf=metadf.drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- titles: string (nullable = true)\n",
      " |-- groups: string (nullable = true)\n",
      " |-- salesranks: double (nullable = true)\n",
      " |-- total_reviews: integer (nullable = true)\n",
      " |-- avg_rating: float (nullable = true)\n",
      " |-- similars_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- categories_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- category_1: string (nullable = true)\n",
      " |-- category_2: string (nullable = true)\n",
      " |-- category_3: string (nullable = true)\n",
      " |-- category_4: string (nullable = true)\n",
      " |-- category_5: string (nullable = true)\n",
      " |-- category_6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using select casting the string columns as the required data type\n",
    "\n",
    "metadf=  metadf.withColumn(\"ID\",col(\"id\").cast(\"int\"))\n",
    "metadf = metadf.withColumn(\"salesranks\", col(\"salesranks\").cast(\"double\"))\n",
    "metadf = metadf.withColumn(\"avg_rating\", col(\"avg_rating\").cast(\"float\"))\n",
    "metadf = metadf.withColumn(\"total_reviews\", col(\"total_reviews\").cast(\"int\"))\n",
    "metadf.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to make a getRecommendation function for the amazon snap dataset.\n",
    "Use the getid(id) function and metadf to make the following functions:\n",
    "1. Make getContext(id) function. \n",
    "The function should return three dataframes \n",
    "first dataframe of id, asin, title, group, similars_list and categories_list \n",
    "second dataframe of id, asin, title, group, similars_list ( similars_list  exploded into multiple rows)\n",
    "third dataframe of id, asin, title, group, categories_list (categories_list exploded into multiple rows)\n",
    "\n",
    "2. Make getSimilars(id) function.\n",
    "The function should call the second dataframe containing the similars_list from getContext(id) function.\n",
    "The function should then match the asins in the similars_list with asins in the metadf and return a dataframe containing id, asin, title, group, categories_list  (categories_list exploded into multiple rows) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Make a getRelevantCategory function as getRelevantCategory(group, top n, score) that gives categories as output \n",
    "It is further divided into two functions: in_category function and out_category function\n",
    "The getRelevantCategory function first calls the getContext(id) function.\n",
    "The getContext(id) function fetches the Asin, title, group, similars_list and categories_list corresponding to the input id.\n",
    "for in_category function:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a getRecommendation(id, group, categories, top_n) function\n",
    "1. call getContext(id), getSimilars(id), getRelevantCategories(group) functions inside the getRecommendation function.\n",
    "2. Input condition:\n",
    "Id cannot be null\n",
    "3. if the input categories is Not Null, match and filter the input categories with that obtained from the getRelevantCategories \n",
    "4. display the top_n asin and title as the top recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For out group recommendations, \n",
    "A.similardf-> filter those ids with similar categories and group \n",
    "B. also, match the title obtained from getcontext with others in the same column using n-gram model\n",
    "C. find out the score as per helpful_percentage *  avg_rating * popularity_percentage. (popularity_percentage= salesrank*100/salesrankmax- scoring on the basis of distribution) \n",
    "higher the score, better the visibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Returns in categories\n",
    "# def getContext(id):\n",
    "#     # get the row matching the input ID\n",
    "#     row = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'groups', 'similars_list', 'categories_list').first()\n",
    "    \n",
    "#     # create first dataframe\n",
    "#     context_df = spark.createDataFrame([row], ['ID', 'asins', 'titles', 'groups', 'similars_list', 'categories_list'])\n",
    "    \n",
    "#     # create second dataframe with similars_list exploded\n",
    "#     similars_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'groups', explode('similars_list').alias('similar_asins'))\n",
    "    \n",
    "#     # create third dataframe with categories_list exploded\n",
    "#     categories_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'groups', explode('categories_list').alias('category'))\n",
    "    \n",
    "#     return context_df, similars_context_df, categories_context_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Returns out categories\n",
    "# def getSimilars(id):\n",
    "#     # get the row matching the input ID\n",
    "#     row = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'groups', 'similars_list', 'categories_list').first()\n",
    "    \n",
    "#     # create a dataframe with similars_list exploded\n",
    "#     similars_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'groups', explode('similars_list').alias('similar_asins'))\n",
    "    \n",
    "#     # match the asins in the similars_list with asins in the metadf\n",
    "#     # matches = metadf.join(similars_context_df, metadf.asins == similars_context_df.similar_asins, 'inner')\n",
    "#     matches = metadf.alias(\"m\").join(similars_context_df.alias(\"s\"), col(\"m.asins\") == col(\"s.similar_asins\"), \"inner\")\n",
    "\n",
    "    \n",
    "#     # select the desired columns and explode the categories_list\n",
    "#     # out_categories_context_df = matches.select('ID', 'asins', 'titles', 'groups', explode('categories_list').alias('category'))\n",
    "#     out_categories_context_df = matches.select(metadf.ID, metadf.asins, metadf.titles, metadf.groups, explode('categories_list').alias('category'))\n",
    "\n",
    "    \n",
    "#     return out_categories_context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRelevantCategory(group):\n",
    "    \n",
    "#     if group is None:\n",
    "#         # use categories_context_df obtained from getContext(id)\n",
    "#         relevant_category_df = categories_context_df\n",
    "#     else:\n",
    "#         # use out_categories_context_df obtained from getSimilars(id)\n",
    "#         out_categories_context_df = getSimilars(id)[3]\n",
    "#         relevant_category_df = out_categories_context_df.filter(col('group') == group)\n",
    "\n",
    "#     # calculate the maximum values for salesrank and total_reviews\n",
    "#     max_salesrank = relevant_category_df.agg({\"salesrank\": \"max\"}).collect()[0][0]\n",
    "#     max_total_reviews = relevant_category_df.agg({\"total_reviews\": \"max\"}).collect()[0][0]\n",
    "\n",
    "#     # calculate the score using min-max normalization\n",
    "#     relevant_category_df = relevant_category_df.withColumn(\"score\", \\\n",
    "#                 (col(\"salesrank\") / max_salesrank + col(\"total_reviews\") / max_total_reviews + col(\"average_rating\")) / 3)\n",
    "\n",
    "#     # count the occurrences of each category\n",
    "#     category_counts = relevant_category_df.groupBy('category').count()\n",
    "\n",
    "#     # sort the categories by count in descending order\n",
    "#     sorted_categories = category_counts.sort(desc('count'))\n",
    "\n",
    "#     # filter the categories by the score threshold\n",
    "#     filtered_categories = sorted_categories.sort(desc('score'))\n",
    "\n",
    "#     return filtered_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRecommendation(id, group, categories, top_n):\n",
    "#     if id is None:\n",
    "#         print(\"Invalid input: id cannot be null\")\n",
    "#         return\n",
    "    \n",
    "#     # get context, similars, and categories dataframes\n",
    "#     context_df, similars_context_df, categories_context_df = getContext(id)\n",
    "#     out_categories_context_df = getSimilars(id)\n",
    "\n",
    "#     # get relevant categories\n",
    "#     relevant_categories_df = getRelevantCategory(group)\n",
    "\n",
    "#     # filter categories by input categories\n",
    "#     if categories is not None:\n",
    "#         categories = [x.lower() for x in categories]\n",
    "#         relevant_categories_df = relevant_categories_df.filter(lower(col('category')).isin(categories))\n",
    "\n",
    "#     # get top n recommendations\n",
    "#     recommendations = context_df.union(similars_context_df).join(\n",
    "#         relevant_categories_df, on='asin', how='inner'\n",
    "#     ).select('asin', 'title', 'score').distinct().sort(desc('score')).limit(top_n)\n",
    "\n",
    "#     # show top n recommendations\n",
    "#     recommendations.show(truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make a getRelevantCategory(group, top_n, score) function as:\n",
    "if input group== Null, consider relevantcategory = categories_context_df obtained from getContext(id)\n",
    "if input group is not Null, relevantcategory = filter by matching the input group with the groups in the out_categories_context_df obtained from getSimilars(id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df= metadf.select('ID', 'asins', 'titles','groups','avg_rating','salesranks','total_reviews', F.explode('categories_list').alias('category')).distinct()\n",
    "cat_df = cat_df.withColumnRenamed(\"category\", \"cat_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantCategory(id, group):\n",
    "    if group is None:\n",
    "        categories_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'salesranks', 'total_reviews', 'avg_rating', 'groups', explode('categories_list').alias('category'))\n",
    "        relevant_category_df = categories_context_df\n",
    "    else:\n",
    "        similars_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'salesranks', 'total_reviews', 'avg_rating', 'groups', 'categories_list', explode('similars_list').alias('similar_asins'))\n",
    "        matches_df = similars_context_df.filter(col('similar_asins').isin(metadf.select('asins').rdd.flatMap(lambda x: x).collect()))\n",
    "        out_categories_context_df = matches_df.select('ID', 'asins', 'titles', 'groups', 'total_reviews', 'avg_rating', 'salesranks', explode('categories_list').alias('category'))\n",
    "        out_categories_context_df = out_categories_context_df.filter((col('groups') == group))\n",
    "        relevant_category_df = out_categories_context_df.distinct()\n",
    "    return relevant_category_df\n",
    "\n",
    "# getRelevantCategory(11, \"Book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendation(id, group, categories, top_n):\n",
    "    if id is None:\n",
    "        print(\"Invalid input: id cannot be null\")\n",
    "        return\n",
    "\n",
    "    # get relevant categories\n",
    "    sorted_categories = getRelevantCategory(id,group)\n",
    "    # sorted_categories.show()\n",
    "    # sorted_categories.printSchema()\n",
    "\n",
    "    # filter the categories in sorted_categories that match with those in cat_df\n",
    "\n",
    "    filtered_categories = sorted_categories.filter(col('category').isin(cat_df.select('cat_category').rdd.flatMap(lambda x: x).collect()))\n",
    "    filtered_categories = filtered_categories.filter((col('groups') == group))\n",
    "\n",
    "    # fetch the input category details from cat_df and append them to the filtered_categories dataframe\n",
    "    for category in categories:\n",
    "        input_category_details = cat_df.filter(col(\"cat_category\") == category)\n",
    "        filtered_categories = filtered_categories.union(input_category_details)\n",
    "\n",
    "    recommendations = filtered_categories.withColumn(\"score\", \\\n",
    "                ((10000000-col(\"salesranks\")) / 10000000 + col(\"total_reviews\") / 100 + col(\"avg_rating\")) / 3)\n",
    "    recommendations= recommendations.sort(desc('score')).limit(top_n)\n",
    "    # recommendations.show(truncate=False)\n",
    "    # recommendations.printSchema()\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "\n",
    "    # # show top n recommendations\n",
    "    # recommendations.show(10, truncate=False)\n",
    "    # relevant_categories_df.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRecommendation(id, group, categories, top_n):\n",
    "#     if id is None:\n",
    "#         print(\"Invalid input: id cannot be null\")\n",
    "#         return\n",
    "\n",
    "#     # get relevant categories\n",
    "#     sorted_categories = getRelevantCategory(id,group)\n",
    "\n",
    "#     # # Add input category to sorted_categories\n",
    "#     # new_row = spark.createDataFrame([(None, None, None, None, None, None, None, categories)], [\"ID\", \"asins\", \"titles\", \"groups\", \"total_reviews\", \"avg_rating\", \"salesranks\", \"category\"])\n",
    "#     # sorted_categories = sorted_categories.union(new_row)\n",
    "    \n",
    "#     schema = StructType([\n",
    "#         StructField(\"ID\", StringType(), True),\n",
    "#         StructField(\"asins\", StringType(), True),\n",
    "#         StructField(\"titles\", StringType(), True),\n",
    "#         StructField(\"groups\", StringType(), True),\n",
    "#         StructField(\"total_reviews\", IntegerType(), True),\n",
    "#         StructField(\"avg_rating\", DoubleType(), True),\n",
    "#         StructField(\"salesranks\", StringType(), True),\n",
    "#         StructField(\"category\", StringType(), True)\n",
    "#     ])\n",
    "#     new_row = spark.createDataFrame([(None, None, None, None, None, None, None, categories)], schema)\n",
    "#     sorted_categories = sorted_categories.union(new_row)\n",
    "\n",
    "#     # filter the categories in sorted_categories that match with those in cat_df\n",
    "#     filtered_categories = sorted_categories.filter(col('category').isin(cat_df.select('cat_category').rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "#     # calculate the score using min-max normalization\n",
    "#     recommendations = filtered_categories.withColumn(\"score\", \\\n",
    "#                 ((10000000-col(\"salesranks\")) / 10000000 + col(\"total_reviews\") / 100 + col(\"avg_rating\")) / 3)\n",
    "#     recommendations= recommendations.sort(desc('score')).limit(top_n)\n",
    "\n",
    "#     return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+-------------+----------+------+---------------+-------------+\n",
      "|ID |asins|titles|salesranks|total_reviews|avg_rating|groups|categories_list|similar_asins|\n",
      "+---+-----+------+----------+-------------+----------+------+---------------+-------------+\n",
      "+---+-----+------+----------+-------------+----------+------+---------------+-------------+\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- titles: string (nullable = true)\n",
      " |-- salesranks: double (nullable = true)\n",
      " |-- total_reviews: integer (nullable = true)\n",
      " |-- avg_rating: float (nullable = true)\n",
      " |-- groups: string (nullable = true)\n",
      " |-- categories_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- similar_asins: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, asins: string, titles: string, groups: string, total_reviews: int, avg_rating: double, salesranks: string, category: string, score: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRecommendation(id='26', group=\"Book\", categories= \"   |Books[283155]|Subjects[1000]|Children's Books[4]|Ages 4-8[2785]|General[170062]\", top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+-------------+----------+----------+--------+-----+\n",
      "|ID |asins|titles|groups|total_reviews|avg_rating|salesranks|category|score|\n",
      "+---+-----+------+------+-------------+----------+----------+--------+-----+\n",
      "+---+-----+------+------+-------------+----------+----------+--------+-----+\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- titles: string (nullable = true)\n",
      " |-- groups: string (nullable = true)\n",
      " |-- total_reviews: integer (nullable = true)\n",
      " |-- avg_rating: float (nullable = true)\n",
      " |-- salesranks: double (nullable = true)\n",
      " |-- category: string (nullable = false)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, asins: string, titles: string, groups: string, total_reviews: int, avg_rating: float, salesranks: double, category: string, score: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRecommendation(id='26', group=\"Book\", categories= \"   |Books[283155]|Subjects[1000]|Children's Books[4]|Ages 4-8[2785]|General[170062]\", top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------+----------+-------------+----------+------------------+\n",
      "|asins     |titles                                |salesranks|total_reviews|avg_rating|score             |\n",
      "+----------+--------------------------------------+----------+-------------+----------+------------------+\n",
      "|0385504209|The Da Vinci Code                     |19.0      |3049         |3.5       |11.663332699999998|\n",
      "|0792151712|Titanic                               |5735.0    |1817         |3.5       |7.5564755         |\n",
      "|1590073991|Path of Daggers (The Wheel of Time, 8)|569310.0  |1688         |3.0       |6.941023          |\n",
      "|1565113306|Fight Club                            |359382.0  |551          |4.5       |3.6580206         |\n",
      "|B0000296JB|Make Yourself                         |2439.0    |545          |4.5       |3.6499187         |\n",
      "+----------+--------------------------------------+----------+-------------+----------+------------------+\n",
      "\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "|asins|titles|salesranks|total_reviews|avg_rating|score|\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "|asins|titles|salesranks|total_reviews|avg_rating|score|\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "+-----+------+----------+-------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getRecommendation(id='11', group=\"Music\", categories= '|Books[283155]|Subjects[1000]|Health, Mind & Body[10]|Alternative Medicine[4696]|General[4701]', top_n=5)\n",
    "getRecommendation(id='19', group=\"DVD\", categories= '   |[139452]|DVD[130]|Genres[404276]|Science Fiction & Fantasy[163431]|Fantasy[163440]', top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from Metadataload import metadataload\n",
    "metadf= metadataload()\n",
    "# metadf.show()\n",
    "cat_df= metadf.select('ID', 'asins', 'titles','groups','avg_rating','salesranks','total_reviews', F.explode('categories_list').alias('category')).distinct()\n",
    "cat_df = cat_df.withColumnRenamed(\"category\", \"cat_category\")\n",
    "def getRelevantCategory(id, group):\n",
    "    if group is None:\n",
    "        categories_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'salesranks', 'total_reviews', 'avg_rating', 'groups', explode('categories_list').alias('category'))\n",
    "        relevant_category_df = categories_context_df\n",
    "    else:\n",
    "        similars_context_df = metadf.filter(col('ID') == id).select('ID', 'asins', 'titles', 'salesranks', 'total_reviews', 'avg_rating', 'groups', 'categories_list', explode('similars_list').alias('similar_asins'))\n",
    "        matches_df = similars_context_df.filter(col('similar_asins').isin(metadf.select('asins').rdd.flatMap(lambda x: x).collect()))\n",
    "        out_categories_context_df = matches_df.select('ID', 'asins', 'titles', 'groups', 'total_reviews', 'avg_rating', 'salesranks', explode('categories_list').alias('category'))\n",
    "        out_categories_context_df = out_categories_context_df.filter((col('groups') == group))\n",
    "        relevant_category_df = out_categories_context_df.distinct()\n",
    "    return relevant_category_df\n",
    "\n",
    "def getRecommendation(id, group, categories, top_n):\n",
    "    if id is None:\n",
    "        print(\"Invalid input: id cannot be null\")\n",
    "        return\n",
    "\n",
    "    # get relevant categories\n",
    "    sorted_categories = getRelevantCategory(id,group)\n",
    "    # filter the categories in sorted_categories that match with those in cat_df\n",
    "    filtered_categories = sorted_categories.filter(col('category').isin(cat_df.select('cat_category').rdd.flatMap(lambda x: x).collect()))\n",
    "    # recommendations= filtered_categories.select('asins', 'titles', 'salesranks', 'total_reviews', 'avg_rating')\n",
    "    # calculate the score using min-max normalization\n",
    "    recommendations = filtered_categories.withColumn(\"score\", \\\n",
    "                ((10000000-col(\"salesranks\")) / 10000000 + col(\"total_reviews\") / 100 + col(\"avg_rating\")) / 3)\n",
    "    recommendations= recommendations.sort(desc('score')).limit(top_n)\n",
    "    # recommendations.show(truncate=False)\n",
    "    # recommendations.printSchema()\n",
    "    recommendations= recommendations.select(\"asins\", \"title\")\n",
    "    recommendations.show(truncate=False)\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, asins: string, titles: string, groups: string, total_reviews: int, avg_rating: float, salesranks: double, category: string, score: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRecommendation(id='19', group=\"DVD\", categories= '   |[139452]|DVD[130]|Genres[404276]|Science Fiction & Fantasy[163431]|Fantasy[163440]', top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations= getRecommendation(id='19', group=\"DVD\", categories= '   |[139452]|DVD[130]|Genres[404276]|Science Fiction & Fantasy[163431]|Fantasy[163440]', top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40580.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 201) (AP-v6p9hhLDJqoL.bfl.com executor driver): java.net.SocketException: Software caused connection abort: recv failed\r\n\tat java.net.SocketInputStream.socketRead0(Native Method)\r\n\tat java.net.SocketInputStream.socketRead(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Software caused connection abort: recv failed\r\n\tat java.net.SocketInputStream.socketRead0(Native Method)\r\n\tat java.net.SocketInputStream.socketRead(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recommendations\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40580.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 201) (AP-v6p9hhLDJqoL.bfl.com executor driver): java.net.SocketException: Software caused connection abort: recv failed\r\n\tat java.net.SocketInputStream.socketRead0(Native Method)\r\n\tat java.net.SocketInputStream.socketRead(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Software caused connection abort: recv failed\r\n\tat java.net.SocketInputStream.socketRead0(Native Method)\r\n\tat java.net.SocketInputStream.socketRead(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\r\n"
     ]
    }
   ],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
